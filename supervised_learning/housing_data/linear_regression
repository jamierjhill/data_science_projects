# Linear regression and feature importance

# Import libraries
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load dataset
data = fetch_california_housing()
X = data.data  # Features
y = data.target  # Target variable (Median house value)
feature_names = data.feature_names  # Feature names

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print results
print("Mean Squared Error:", mse)
print("R^2 Score:", r2)

# Display some predictions vs actual values
results = pd.DataFrame({"Actual": y_test[:10], "Predicted": y_pred[:10]})
print("\nSample Predictions:\n", results)

# Feature importance (coefficients for Linear Regression)
coefficients = model.coef_
feature_importance = pd.DataFrame({
    "Feature": feature_names,
    "Importance": coefficients
}).sort_values(by="Importance", key=abs, ascending=False)

print("\nFeature Importance:\n", feature_importance)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance["Feature"], feature_importance["Importance"], color="skyblue")
plt.xlabel("Coefficient Value")
plt.title("Feature Importance in Linear Regression")
plt.gca().invert_yaxis()
plt.show()
